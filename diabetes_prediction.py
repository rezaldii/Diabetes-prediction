# -*- coding: utf-8 -*-
"""Diabetes-prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/104jJfZFoO8pNNzcBAmCVc_Dc4Cvj2AHM

# Proyek Predictive Analytics: Diabetes-prediction
- Nama: Rezaldi
- Email: rezaldi30082003@students.amikom.ac.id
- Id Dicoding: rezaldi_20113717
- Dataset: https://www.kaggle.com/datasets/iammustafatz/diabetes-prediction-dataset

## Import library
"""

import pandas as pd
import matplotlib.pyplot as plt
import scipy.stats as stats
from scipy.stats import chi2_contingency
import seaborn as sns
from sklearn.ensemble import IsolationForest
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import cross_val_score

"""## Data Understanding

### Data Collection
"""

# Load the dataset into a DataFrame
data = pd.read_csv('diabetes_prediction_dataset.csv')
# Display the first 5 rows of the data
data.head()

"""### Data Description"""

# Display the shape of the data
print(f'The dataset contains {data.shape[0]} rows and {data.shape[1]} columns.')

# Display the data types of each column
print('\nThe data types of each column are:')
print(data.dtypes)

# Display summary statistics for the numerical columns
data.describe()

# Display the number of missing values in each column
print('Number of missing values in each column:')
print(data.isnull().sum())

# Display the number of unique values in each column
print('\nNumber of unique values in each column:')
print(data.nunique())

# Display the frequency counts for categorical columns
for col in data.select_dtypes(include='object').columns:
    print(f'\nFrequency counts for {col}:')
    print(data[col].value_counts())

"""### Exploratory Data Analysis (EDA)

#### Data visualization
"""

# Grafik batang untuk variabel jenis kelamin (gender)
gender_counts = data['gender'].value_counts()
plt.bar(gender_counts.index, gender_counts.values)
plt.xlabel('Gender')
plt.ylabel('Count')
plt.title('Frequency of Gender')
plt.show()

# Grafik batang untuk variabel riwayat merokok (smoking_history)
smoking_counts = data['smoking_history'].value_counts()
plt.bar(smoking_counts.index, smoking_counts.values)
plt.xlabel('Smoking History')
plt.ylabel('Count')
plt.title('Frequency of Smoking History')
plt.show()

# Histogram untuk variabel usia (age)
plt.hist(data['age'], bins=10)
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.title('Distribution of Age')
plt.show()

# Histogram untuk variabel indeks massa tubuh (bmi)
plt.hist(data['bmi'], bins=10)
plt.xlabel('BMI')
plt.ylabel('Frequency')
plt.title('Distribution of BMI')
plt.show()

# Histogram untuk variabel tingkat glukosa darah (blood_glucose_level)
plt.hist(data['blood_glucose_level'], bins=10)
plt.xlabel('Blood Glucose Level')
plt.ylabel('Frequency')
plt.title('Distribution of Blood Glucose Level')
plt.show()

# Boxplot untuk variabel usia (age)
plt.boxplot(data['age'])
plt.xlabel('Age')
plt.title('Boxplot of Age')
plt.show()

# Boxplot untuk variabel indeks massa tubuh (bmi)
plt.boxplot(data['bmi'])
plt.xlabel('BMI')
plt.title('Boxplot of BMI')
plt.show()

# Boxplot untuk variabel tingkat glukosa darah (blood_glucose_level)
plt.boxplot(data['blood_glucose_level'])
plt.xlabel('Blood Glucose Level')
plt.title('Boxplot of Blood Glucose Level')
plt.show()

# Scatterplot untuk hubungan antara usia (age) dan indeks massa tubuh (bmi)
plt.scatter(data['age'], data['bmi'])
plt.xlabel('Age')
plt.ylabel('BMI')
plt.title('Scatterplot of Age vs BMI')
plt.show()

"""#### Pengujian hipotesis statistik"""

# Pisahkan data menjadi dua kelompok berdasarkan diabetes
diabetes_group = data[data['diabetes'] == 1]
non_diabetes_group = data[data['diabetes'] == 0]

# Lakukan uji t
t_statistic, p_value = stats.ttest_ind(diabetes_group['bmi'], non_diabetes_group['bmi'], equal_var=False)

# Tampilkan hasil uji
print(f'T-Statistic: {t_statistic}')
print(f'P-Value: {p_value}')

# Ambil alpha level (biasanya 0.05)
alpha = 0.05

# Tentukan apakah hasil uji statistik signifikan
if p_value < alpha:
    print('Terdapat perbedaan signifikan dalam rata-rata BMI antara pasien diabetes dan non-diabetes.')
else:
    print('Tidak terdapat perbedaan signifikan dalam rata-rata BMI antara pasien diabetes dan non-diabetes.')

# Pisahkan data menjadi kelompok berdasarkan riwayat merokok
groups = [data[data['smoking_history'] == 'never']['bmi'],
          data[data['smoking_history'] == 'former']['bmi'],
          data[data['smoking_history'] == 'current']['bmi']]

# Lakukan uji ANOVA
f_statistic, p_value = stats.f_oneway(*groups)

# Tampilkan hasil uji
print(f'F-Statistic: {f_statistic}')
print(f'P-Value: {p_value}')

# Ambil alpha level (biasanya 0.05)
alpha = 0.05

# Tentukan apakah hasil uji statistik signifikan
if p_value < alpha:
    print('Terdapat perbedaan signifikan dalam rata-rata BMI antara kelompok berbeda berdasarkan riwayat merokok.')
else:
    print('Tidak terdapat perbedaan signifikan dalam rata-rata BMI antara kelompok berbeda berdasarkan riwayat merokok.')

# Membuat tabel kontingensi
contingency_table = pd.crosstab(data['gender'], data['diabetes'])
print(contingency_table)

# Melakukan Pengujian Chi-square
chi2, p, dof, expected = chi2_contingency(contingency_table)
print(f"Chi-square Value: {chi2}")
print(f"P-Value: {p}")
print(f"Degrees of Freedom: {dof}")
print("Expected Frequencies Table:")
print(expected)

# Hitung korelasi antara usia (age) dan indeks massa tubuh (bmi) menggunakan metode Pearson
correlation = data['age'].corr(data['bmi'], method='pearson')

# Visualisasi scatterplot antara usia (age) dan indeks massa tubuh (bmi)
plt.figure(figsize=(8, 6))
sns.scatterplot(x='age', y='bmi', data=data)
plt.xlabel('Age')
plt.ylabel('BMI')
plt.title(f'Scatterplot of Age vs BMI (Correlation: {correlation:.2f})')
plt.show()

# Menampilkan nilai korelasi
print(f'Correlation between age and BMI: {correlation:.2f}')

"""#### Deteksi anomali"""

# Pilih variabel numerik yang ingin Anda deteksi anomali
numerical_columns = ['age', 'bmi', 'HbA1c_level', 'blood_glucose_level']

# Lakukan deteksi anomali untuk setiap variabel numerik
for column in numerical_columns:
    # Hitung kuartil pertama (Q1) dan kuartil ketiga (Q3)
    Q1 = data[column].quantile(0.25)
    Q3 = data[column].quantile(0.75)

    # Hitung IQR (Interquartile Range)
    IQR = Q3 - Q1

    # Tentukan batas bawah dan batas atas untuk deteksi anomali
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Temukan data yang di luar batas atas atau batas bawah
    anomalies = data[(data[column] < lower_bound) | (data[column] > upper_bound)]

    # Tampilkan data anomali
    print(f"Anomalous values in {column}:")
    print(anomalies)

# Select the features you want to use for anomaly detection (e.g., age and bmi)
features = ['age', 'bmi']

# Create a DataFrame with selected features
X = data[features]

# Initialize the Isolation Forest model
clf = IsolationForest(contamination=0.05, random_state=42)

# Fit the model to the data and predict anomalies
data['anomaly'] = clf.fit_predict(X)

# Anomalies are labeled as -1, so we filter the data to get only anomalies
anomalies = data[data['anomaly'] == -1]

# Print the anomalies
print("Anomalous values:")
print(anomalies)

"""### Verifikasi Kualitas Data"""

# Cek jumlah data yang duplikat
duplicate_count = data.duplicated().sum()

# Tampilkan jumlah data yang duplikat
print(f"Jumlah data yang duplikat: {duplicate_count}")

# Hapus data yang duplikat
data = data.drop_duplicates()

# Tampilkan bentuk baru dari dataset setelah menghapus data yang duplikat
print(f"Bentuk dataset setelah menghapus data yang duplikat: {data.shape}")

# Pilih variabel numerik yang ingin Anda periksa dan atasi outlier (misalnya, age, bmi, HbA1c_level)
numerical_columns = ['age', 'bmi', 'HbA1c_level']

# Lakukan penanganan outlier untuk setiap variabel numerik
for column in numerical_columns:
    # Hitung kuartil pertama (Q1) dan kuartil ketiga (Q3)
    Q1 = data[column].quantile(0.25)
    Q3 = data[column].quantile(0.75)

    # Hitung IQR (Interquartile Range)
    IQR = Q3 - Q1

    # Tentukan batas bawah dan batas atas untuk deteksi outlier
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Gantikan nilai outlier dengan nilai batas atas atau batas bawah
    data[column] = np.where(data[column] < lower_bound, lower_bound, data[column])
    data[column] = np.where(data[column] > upper_bound, upper_bound, data[column])

# Check for inconsistencies in age (e.g., negative values)
inconsistent_age = data[data['age'] < 0]

# Check for inconsistencies in gender (e.g., unexpected values)
unexpected_gender = data[~data['gender'].isin(['Male', 'Female'])]

# Print the records with inconsistencies
print("Inconsistent Age:")
print(inconsistent_age)

print("\nUnexpected Gender:")
print(unexpected_gender)

# Mengganti "Other" dengan NaN dalam kolom "gender"
data['gender'] = data['gender'].replace('Other', np.nan)

# Menyimpan dataset yang telah diperbarui
data.to_csv('diabetes_prediction_dataset_updated.csv', index=False)

"""## Data Preparation"""

# Pilih fitur numerik yang ingin distandarisasi
numerical_features = ['age', 'bmi', 'HbA1c_level', 'blood_glucose_level']

# Inisialisasi Standard Scaler
scaler = StandardScaler()

# Lakukan standarisasi pada fitur-fitur tersebut
data[numerical_features] = scaler.fit_transform(data[numerical_features])

# Tampilkan data setelah standarisasi
print(data.head())

# Membaca dataset yang telah diubah
data = pd.read_csv('diabetes_prediction_dataset_updated.csv')

# Melakukan one-hot encoding untuk kolom "gender" dan "smoking_history"
data_encoded = pd.get_dummies(data, columns=['gender', 'smoking_history'], drop_first=True)

# Menampilkan lima baris pertama dari dataset yang telah di-encode
print(data_encoded.head())

# Simpan dataset yang telah di-encode
data_encoded.to_csv('diabetes_prediction_dataset_encoded.csv', index=False)

# Membaca dataset yang telah diencode
data_encoded = pd.read_csv('diabetes_prediction_dataset_encoded.csv')

# Menghitung korelasi antara fitur dan target (diabetes)
correlations = data_encoded.corr()['diabetes'].abs().sort_values(ascending=False)

# Menentukan ambang batas korelasi yang relevan (misalnya, 0.1)
relevant_features = correlations[correlations > 0.1].index.tolist()

# Menampilkan fitur-fitur yang memiliki korelasi relevan
print("Fitur-fitur yang memiliki korelasi relevan dengan diabetes:")
print(relevant_features)

# Memvisualisasikan matriks korelasi
corr_matrix = data_encoded[relevant_features].corr()
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Matriks Korelasi Fitur-fitur yang Relevan')
plt.show()

# Memisahkan fitur (X) dan target (y)
X = data_encoded[['blood_glucose_level', 'HbA1c_level', 'age', 'bmi', 'hypertension', 'heart_disease']]
y = data_encoded['diabetes']

# Memisahkan data menjadi set pelatihan dan set tes (misalnya, 80% pelatihan dan 20% tes)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Menampilkan jumlah sampel dalam set pelatihan dan set tes
print("Jumlah sampel dalam set pelatihan:", len(X_train))
print("Jumlah sampel dalam set tes:", len(X_test))

"""## Modeling"""

# Inisialisasi model Regresi Logistik
logistic_regression_model = LogisticRegression(random_state=42)

# Melatih model pada set pelatihan
logistic_regression_model.fit(X_train, y_train)

# Memprediksi kelas target pada set tes
y_pred = logistic_regression_model.predict(X_test)

# Menghitung akurasi model
accuracy = accuracy_score(y_test, y_pred)
print("Akurasi model Regresi Logistik:", accuracy)

# Menampilkan laporan klasifikasi
classification_rep = classification_report(y_test, y_pred)
print("Laporan Klasifikasi:\n", classification_rep)

# Menghitung dan menampilkan matriks konfusi
conf_matrix = confusion_matrix(y_test, y_pred)
print("Matriks Konfusi:\n", conf_matrix)

# Daftar hyperparameter yang akan disetel
param_grid = {
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'criterion': ['gini', 'entropy']
}

# Inisialisasi model Pohon Keputusan
decision_tree_model = DecisionTreeClassifier(random_state=42)

# Inisialisasi Grid Search
grid_search = GridSearchCV(estimator=decision_tree_model, param_grid=param_grid,
                           scoring='accuracy', cv=5, n_jobs=-1)

# Melatih model dengan Grid Search
grid_search.fit(X_train, y_train)

# Mendapatkan model terbaik setelah penyetelan
best_decision_tree_model = grid_search.best_estimator_

# Memprediksi kelas target pada set tes
y_pred_best_dt = best_decision_tree_model.predict(X_test)

# Menghitung akurasi model Pohon Keputusan setelah penyetelan
accuracy_best_dt = accuracy_score(y_test, y_pred_best_dt)
print("Akurasi model Pohon Keputusan setelah penyetelan:", accuracy_best_dt)

# Menampilkan laporan klasifikasi setelah penyetelan
classification_rep_best_dt = classification_report(y_test, y_pred_best_dt)
print("Laporan Klasifikasi Pohon Keputusan setelah penyetelan:\n", classification_rep_best_dt)

# Menghitung dan menampilkan matriks konfusi setelah penyetelan
conf_matrix_best_dt = confusion_matrix(y_test, y_pred_best_dt)
print("Matriks Konfusi Pohon Keputusan setelah penyetelan:\n", conf_matrix_best_dt)

# Menentukan grid parameter yang akan diuji
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Inisialisasi model Random Forest
random_forest_model = RandomForestClassifier(random_state=42)

# Membuat objek GridSearchCV
grid_search = GridSearchCV(estimator=random_forest_model, param_grid=param_grid,
                           scoring='accuracy', cv=3, verbose=2, n_jobs=-1)

# Melakukan penyetelan hyperparameter pada set pelatihan
grid_search.fit(X_train, y_train)

# Menampilkan parameter terbaik
print("Parameter terbaik:", grid_search.best_params_)

# Membuat model dengan parameter terbaik
best_rf_model = grid_search.best_estimator_

# Memprediksi kelas target pada set tes
y_pred_best_rf = best_rf_model.predict(X_test)

# Menghitung akurasi model Random Forest yang ditingkatkan
accuracy_best_rf = accuracy_score(y_test, y_pred_best_rf)
print("Akurasi model Random Forest yang ditingkatkan:", accuracy_best_rf)

# Menampilkan laporan klasifikasi
classification_rep_best_rf = classification_report(y_test, y_pred_best_rf)
print("Laporan Klasifikasi Random Forest yang ditingkatkan:\n", classification_rep_best_rf)

# Menghitung dan menampilkan matriks konfusi
conf_matrix_best_rf = confusion_matrix(y_test, y_pred_best_rf)
print("Matriks Konfusi Random Forest yang ditingkatkan:\n", conf_matrix_best_rf)

"""## Evaluation"""

# Evaluasi model Regresi Logistik
accuracy_lr = accuracy_score(y_test, y_pred)
precision_lr = precision_score(y_test, y_pred)
recall_lr = recall_score(y_test, y_pred)
f1_score_lr = f1_score(y_test, y_pred)

print("Evaluasi model Regresi Logistik:")
print("Akurasi:", accuracy_lr)
print("Presisi (Precision):", precision_lr)
print("Recall:", recall_lr)
print("F1-Score:", f1_score_lr)

# Evaluasi model Pohon Keputusan (setelah penyetelan)
accuracy_dt = accuracy_score(y_test, y_pred_best_dt)
precision_dt = precision_score(y_test, y_pred_best_dt)
recall_dt = recall_score(y_test, y_pred_best_dt)
f1_score_dt = f1_score(y_test, y_pred_best_dt)

print("\nEvaluasi model Pohon Keputusan (setelah penyetelan):")
print("Akurasi:", accuracy_dt)
print("Presisi (Precision):", precision_dt)
print("Recall:", recall_dt)
print("F1-Score:", f1_score_dt)

# Evaluasi model Random Forest (setelah penyetelan)
accuracy_rf = accuracy_score(y_test, y_pred_best_rf)
precision_rf = precision_score(y_test, y_pred_best_rf)
recall_rf = recall_score(y_test, y_pred_best_rf)
f1_score_rf = f1_score(y_test, y_pred_best_rf)

print("\nEvaluasi model Random Forest (setelah penyetelan):")
print("Akurasi:", accuracy_rf)
print("Presisi (Precision):", precision_rf)
print("Recall:", recall_rf)
print("F1-Score:", f1_score_rf)

# Regresi Logistik
cv_scores_lr = cross_val_score(logistic_regression_model, X, y, cv=5)
avg_cv_score_lr = cv_scores_lr.mean()

# Pohon Keputusan (setelah penyetelan)
cv_scores_dt = cross_val_score(best_decision_tree_model, X, y, cv=5)
avg_cv_score_dt = cv_scores_dt.mean()

# Random Forest (setelah penyetelan)
cv_scores_rf = cross_val_score(best_rf_model, X, y, cv=5)
avg_cv_score_rf = cv_scores_rf.mean()

print("Evaluasi model dengan Cross-Validation:")
print("Regresi Logistik (Rata-rata CV Score):", avg_cv_score_lr)
print("Pohon Keputusan (Rata-rata CV Score):", avg_cv_score_dt)
print("Random Forest (Rata-rata CV Score):", avg_cv_score_rf)